{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\ale_py\\roms\\__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'shape'\n  In call to configurable 'wrap_env' (<function wrap_env at 0x0000020CB375C550>)\n  In call to configurable 'load' (<function load at 0x0000020CB373DB80>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\OneDrive\\book\\Atari_game\\Thesis\\tfagent.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000002?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000002?line=2'>3</a>\u001b[0m env_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mALE/Breakout-v5\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000002?line=3'>4</a>\u001b[0m env \u001b[39m=\u001b[39m suite_gym\u001b[39m.\u001b[39;49mload(env_name, gym_env_wrappers\u001b[39m=\u001b[39;49m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000002?line=4'>5</a>\u001b[0m     NoopResetEnv,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000002?line=5'>6</a>\u001b[0m     ResizeObservation\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000002?line=6'>7</a>\u001b[0m ])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[1;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[0;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\tf_agents\\environments\\suite_gym.py:90\u001b[0m, in \u001b[0;36mload\u001b[1;34m(environment_name, discount, max_episode_steps, gym_env_wrappers, env_wrappers, spec_dtype_map, gym_kwargs, render_kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m max_episode_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gym_spec\u001b[39m.\u001b[39mmax_episode_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m   max_episode_steps \u001b[39m=\u001b[39m gym_spec\u001b[39m.\u001b[39mmax_episode_steps\n\u001b[1;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m wrap_env(\n\u001b[0;32m     91\u001b[0m     gym_env,\n\u001b[0;32m     92\u001b[0m     discount\u001b[39m=\u001b[39;49mdiscount,\n\u001b[0;32m     93\u001b[0m     max_episode_steps\u001b[39m=\u001b[39;49mmax_episode_steps,\n\u001b[0;32m     94\u001b[0m     gym_env_wrappers\u001b[39m=\u001b[39;49mgym_env_wrappers,\n\u001b[0;32m     95\u001b[0m     env_wrappers\u001b[39m=\u001b[39;49menv_wrappers,\n\u001b[0;32m     96\u001b[0m     spec_dtype_map\u001b[39m=\u001b[39;49mspec_dtype_map,\n\u001b[0;32m     97\u001b[0m     render_kwargs\u001b[39m=\u001b[39;49mrender_kwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[1;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[0;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\tf_agents\\environments\\suite_gym.py:143\u001b[0m, in \u001b[0;36mwrap_env\u001b[1;34m(gym_env, discount, max_episode_steps, gym_env_wrappers, time_limit_wrapper, env_wrappers, spec_dtype_map, auto_reset, render_kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m\"\"\"Wraps given gym environment with TF Agent's GymWrapper.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[39mNote that by default a TimeLimit wrapper is used to limit episode lengths\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39m  A PyEnvironment instance.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m wrapper \u001b[39min\u001b[39;00m gym_env_wrappers:\n\u001b[1;32m--> 143\u001b[0m   gym_env \u001b[39m=\u001b[39m wrapper(gym_env)\n\u001b[0;32m    144\u001b[0m env \u001b[39m=\u001b[39m gym_wrapper\u001b[39m.\u001b[39mGymWrapper(\n\u001b[0;32m    145\u001b[0m     gym_env,\n\u001b[0;32m    146\u001b[0m     discount\u001b[39m=\u001b[39mdiscount,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m     render_kwargs\u001b[39m=\u001b[39mrender_kwargs,\n\u001b[0;32m    150\u001b[0m )\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m max_episode_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m max_episode_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'shape'\n  In call to configurable 'wrap_env' (<function wrap_env at 0x0000020CB375C550>)\n  In call to configurable 'load' (<function load at 0x0000020CB373DB80>)"
     ]
    }
   ],
   "source": [
    "from Atari_Warppers import NoopResetEnv, NormalizedEnv, ResizeObservation, SyncVectorEnv, ClipRewardEnv, EpisodicLifeEnv, FireResetEnv\n",
    "import gym\n",
    "env_name = 'ALE/Breakout-v5'\n",
    "env = suite_gym.load(env_name, gym_env_wrappers=[\n",
    "    NoopResetEnv,\n",
    "    ResizeObservation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACu0lEQVR4nO3dsW0TYRiA4QS5RkxARcEIEQNYLpjGEzCBx0AMQGGloEQZBlEgRJEiygL+IZZ9d/bL85Sn090vvfl8v+SzcnMDAMDZ3c55s91u989zttvtbOcfa+rrj+51yjVfnWMxXK7VUjeec1Jfcv6xzjWpUzPBcYtN8LUbfSpc2mSb4DgTfITRdE7xjD8XExy32AQf+1c/9flLXXNqJhjgYt1e43OFl/MMjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4bvjKzqW9/snfjb72NcFxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3PDF94fNZs51cKLvg+MmOE7gOIHjBI4TOG64i35692vOdTARExwncJzAcQLHCRw33EX/fP1nznUwERMcJ3CcwHECxwkcN95Fv3+ccx2c6sfhwyY4TuA4geMEjhM4briL/vz0ds51cKL14LgJjhM4TuA4geMEjhvuoh+/fJpxGZxsffj3hSY4TuA4geMEjhM4briL/ra/m3MdnOjj2j+n/C8JHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3Orrm99Lr+E6PGw2R51/t99PtJKDPtzfHzxuguMEjhM4brX0Aq7GzM/UczHBcSY44ko/YAAAAAAAAFjUMyEkRxlHStXAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step:\n",
      "TimeStep(step_type=array(0), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8))\n",
      "Next time step:\n",
      "TimeStep(step_type=array(1), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8))\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "\n",
    "action = np.array(1, dtype=np.int32)\n",
    "\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf_agents.networks.sequential.Sequential object at 0x0000020CB5976DF0>\n"
     ]
    }
   ],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu)\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None)\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "print(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected q_network to emit a floating point tensor with inner dims (4,); but saw network output spec: TensorSpec(shape=(210, 160, 4), dtype=tf.float32, name=None)\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\OneDrive\\book\\Atari_game\\Thesis\\tfagent.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=2'>3</a>\u001b[0m train_step_counter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(\u001b[39m0\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=4'>5</a>\u001b[0m agent \u001b[39m=\u001b[39m dqn_agent\u001b[39m.\u001b[39;49mDqnAgent(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=5'>6</a>\u001b[0m     train_env\u001b[39m.\u001b[39;49mtime_step_spec(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=6'>7</a>\u001b[0m     train_env\u001b[39m.\u001b[39;49maction_spec(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=7'>8</a>\u001b[0m     q_network\u001b[39m=\u001b[39;49mq_net,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=8'>9</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=9'>10</a>\u001b[0m     td_errors_loss_fn\u001b[39m=\u001b[39;49mcommon\u001b[39m.\u001b[39;49melement_wise_squared_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=10'>11</a>\u001b[0m     train_step_counter\u001b[39m=\u001b[39;49mtrain_step_counter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/OneDrive/book/Atari_game/Thesis/tfagent.ipynb#ch0000009?line=12'>13</a>\u001b[0m agent\u001b[39m.\u001b[39minitialize()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m scope_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m in scope \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(scope_str) \u001b[39mif\u001b[39;00m scope_str \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1604\u001b[0m err_str \u001b[39m=\u001b[39m err_str\u001b[39m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[1;32m-> 1605\u001b[0m utils\u001b[39m.\u001b[39;49maugment_exception_message_and_reraise(e, err_str)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[1;34m(exception, message)\u001b[0m\n\u001b[0;32m     39\u001b[0m proxy \u001b[39m=\u001b[39m ExceptionProxy()\n\u001b[0;32m     40\u001b[0m ExceptionProxy\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(exception)\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[39mraise\u001b[39;00m proxy\u001b[39m.\u001b[39mwith_traceback(exception\u001b[39m.\u001b[39m__traceback__) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\gin\\config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1579\u001b[0m new_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m   1581\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1582\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49mnew_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_kwargs)\n\u001b[0;32m   1583\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m   err_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py:240\u001b[0m, in \u001b[0;36mDqnAgent.__init__\u001b[1;34m(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, name)\u001b[0m\n\u001b[0;32m    235\u001b[0m   target_q_network\u001b[39m.\u001b[39mcreate_variables(net_observation_spec)\n\u001b[0;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_q_network \u001b[39m=\u001b[39m common\u001b[39m.\u001b[39mmaybe_copy_target_network_with_checks(\n\u001b[0;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_q_network, target_q_network, input_spec\u001b[39m=\u001b[39mnet_observation_spec,\n\u001b[0;32m    238\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTargetQNetwork\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 240\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_network_output(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_q_network, \u001b[39m'\u001b[39;49m\u001b[39mq_network\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_network_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_q_network, \u001b[39m'\u001b[39m\u001b[39mtarget_q_network\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    243\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epsilon_greedy \u001b[39m=\u001b[39m epsilon_greedy\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\tf_agents\\agents\\dqn\\dqn_agent.py:320\u001b[0m, in \u001b[0;36mDqnAgent._check_network_output\u001b[1;34m(self, net, label)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_network_output\u001b[39m(\u001b[39mself\u001b[39m, net, label):\n\u001b[0;32m    311\u001b[0m   \u001b[39m\"\"\"Check outputs of q_net and target_q_net against expected shape.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[39m  Subclasses that require different q_network outputs should override\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m    label: A label to print in case of a mismatch.\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m   network_utils\u001b[39m.\u001b[39;49mcheck_single_floating_network_output(\n\u001b[0;32m    321\u001b[0m       net\u001b[39m.\u001b[39;49mcreate_variables(),\n\u001b[0;32m    322\u001b[0m       expected_output_shape\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_actions,),\n\u001b[0;32m    323\u001b[0m       label\u001b[39m=\u001b[39;49mlabel)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\guanghongli\\lib\\site-packages\\tf_agents\\networks\\utils.py:38\u001b[0m, in \u001b[0;36mcheck_single_floating_network_output\u001b[1;34m(output_spec, expected_output_shape, label)\u001b[0m\n\u001b[0;32m     34\u001b[0m expected_output_shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mint\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m expected_output_shape)\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(output_spec, tf\u001b[39m.\u001b[39mTensorSpec)\n\u001b[0;32m     36\u001b[0m         \u001b[39mand\u001b[39;00m output_spec\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m expected_output_shape\n\u001b[0;32m     37\u001b[0m         \u001b[39mand\u001b[39;00m output_spec\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating):\n\u001b[1;32m---> 38\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     39\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to emit a floating point tensor with inner dims \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     40\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m; but saw network output spec: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     41\u001b[0m       \u001b[39m.\u001b[39mformat(label, expected_output_shape, output_spec))\n",
      "\u001b[1;31mValueError\u001b[0m: Expected q_network to emit a floating point tensor with inner dims (4,); but saw network output spec: TensorSpec(shape=(210, 160, 4), dtype=tf.float32, name=None)\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('guanghongli')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e45e94b5413f1f809cf64fb9d3aa982429c9312fb3168c2283a62202e9b09678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
